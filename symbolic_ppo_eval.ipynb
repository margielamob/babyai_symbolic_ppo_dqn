{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4m26me_n-GZ"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "from minigrid.wrappers import ImgObsWrapper\n",
        "from minigrid.core.constants import COLOR_NAMES\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "# Copied from our training script will be used for evaluation\n",
        "\n",
        "class GoToRedBallLogicWrapper(gym.Wrapper):\n",
        "    def __init__(self, env: gym.Env, violation_penalty: float = -0.5):\n",
        "        super().__init__(env)\n",
        "        self.penalty = violation_penalty\n",
        "        try:\n",
        "            self.red_color_index = COLOR_NAMES.index('red')\n",
        "        except ValueError:\n",
        "             print(f\"Error: 'red' not found in COLOR_NAMES list: {COLOR_NAMES}\")\n",
        "             self.red_color_index = 0\n",
        "        self.red_ball_pos = None\n",
        "        assert hasattr(self.env.unwrapped, 'actions'), \"Environment must have an 'actions' attribute\"\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        grid = self.env.unwrapped.grid\n",
        "        self.red_ball_pos = None\n",
        "        ball_found_debug = False\n",
        "\n",
        "        for i in range(grid.width):\n",
        "            for j in range(grid.height):\n",
        "                cell = grid.get(i, j)\n",
        "                if cell:\n",
        "                    raw_color_attr = getattr(cell, 'color', None)\n",
        "                    cell_type = getattr(cell, 'type', 'N/A')\n",
        "                    cell_color_idx = -1\n",
        "                    if isinstance(raw_color_attr, str):\n",
        "                        try: cell_color_idx = COLOR_NAMES.index(raw_color_attr)\n",
        "                        except ValueError: pass\n",
        "                    elif isinstance(raw_color_attr, int):\n",
        "                         if 0 <= raw_color_attr < len(COLOR_NAMES):\n",
        "                             cell_color_idx = raw_color_attr\n",
        "\n",
        "                    if hasattr(cell, 'type') and cell.type == 'ball':\n",
        "                         ball_found_debug = True\n",
        "                         if cell_color_idx == self.red_color_index:\n",
        "                             self.red_ball_pos = (i, j)\n",
        "                             break\n",
        "            if self.red_ball_pos:\n",
        "                 break\n",
        "\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        info['logic_violation'] = None\n",
        "        unwrapped_env = self.env.unwrapped\n",
        "\n",
        "        if action == unwrapped_env.actions.pickup:\n",
        "            fx, fy = unwrapped_env.front_pos\n",
        "            cell_in_front = unwrapped_env.grid.get(fx, fy)\n",
        "            if cell_in_front and hasattr(cell_in_front, 'type') and cell_in_front.type == 'ball':\n",
        "                 raw_color_attr = getattr(cell_in_front, 'color', None)\n",
        "                 cell_color_idx = -1\n",
        "                 if isinstance(raw_color_attr, str):\n",
        "                     try: cell_color_idx = COLOR_NAMES.index(raw_color_attr)\n",
        "                     except ValueError: pass\n",
        "                 elif isinstance(raw_color_attr, int):\n",
        "                     if 0 <= raw_color_attr < len(COLOR_NAMES):\n",
        "                        cell_color_idx = raw_color_attr\n",
        "\n",
        "                 if cell_color_idx != self.red_color_index:\n",
        "                    reward += self.penalty\n",
        "                    info['logic_violation'] = 'pickup_wrong_ball'\n",
        "\n",
        "        if action == unwrapped_env.actions.done:\n",
        "            agent_pos = tuple(unwrapped_env.agent_pos)\n",
        "            if self.red_ball_pos is None:\n",
        "                 reward += self.penalty\n",
        "                 info['logic_violation'] = 'premature_done_no_target'\n",
        "            elif agent_pos != self.red_ball_pos:\n",
        "                reward += self.penalty\n",
        "                info['logic_violation'] = 'premature_done'\n",
        "\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "class MiniGridCNN(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 64):\n",
        "        super().__init__(observation_space, features_dim)\n",
        "        n_input_channels = observation_space.shape[0]\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.as_tensor(observation_space.sample()[None]).float()\n",
        "            n_flatten = self.cnn(dummy_input).shape[1]\n",
        "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
        "\n",
        "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
        "        return self.linear(self.cnn(observations.float()))\n",
        "\n",
        "def make_env(env_id, violation_penalty=-0.5, seed=0, render_mode=None):\n",
        "    \"\"\" Note: seed is mostly handled by VecEnv wrappers now \"\"\"\n",
        "    def _init():\n",
        "        env_kwargs = {'render_mode': render_mode} if render_mode else {}\n",
        "        env = gym.make(env_id, **env_kwargs)\n",
        "        env = ImgObsWrapper(env)\n",
        "        env = GoToRedBallLogicWrapper(env, violation_penalty=violation_penalty)\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "ENV_ID = 'BabyAI-GoToRedBallGrey-v0'\n",
        "MODEL_SAVE_PATH = \"ppo_goto_redball_logic_custom_cnn.zip\"\n",
        "VIOLATION_PENALTY = -0.5\n",
        "FEATURES_DIM = 64\n",
        "\n",
        "# evaluation script\n",
        "if __name__ == \"__main__\":\n",
        "    N_EVAL_EPISODES = 50\n",
        "    EVAL_NUM_ENVS = 1\n",
        "\n",
        "    print(f\"\\n--- Starting Evaluation ---\")\n",
        "    print(f\"Loading model: {MODEL_SAVE_PATH}\")\n",
        "    print(f\"Evaluating on env: {ENV_ID} for {N_EVAL_EPISODES} episodes.\")\n",
        "\n",
        "    eval_env = DummyVecEnv([make_env(ENV_ID, violation_penalty=VIOLATION_PENALTY, seed=1000)]) # Use a fixed seed offset for eval\n",
        "\n",
        "    try:\n",
        "        model = PPO.load(MODEL_SAVE_PATH, env=eval_env)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        print(\"Ensure the model file exists and was saved correctly.\")\n",
        "        eval_env.close()\n",
        "        exit()\n",
        "\n",
        "    mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=N_EVAL_EPISODES, deterministic=True, render=False)\n",
        "\n",
        "    print(f\"\\n--- Evaluation Results ---\")\n",
        "    print(f\"Mean reward over {N_EVAL_EPISODES} episodes: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "    print(\"Note: 'Reward' here includes penalties from the wrapper.\")\n",
        "    print(\"Higher reward generally means better performance (fewer penalties, reaching goal faster).\")\n",
        "\n",
        "    eval_env.close()\n",
        "    print(\"\\nEvaluation finished.\")\n",
        "\n",
        "    # Visualise and record the game into a video format\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "from IPython import display as ipythondisplay\n",
        "from pathlib import Path\n",
        "import base64\n",
        "\n",
        "def show_video(video_path, video_width = 600):\n",
        "    video_file = open(video_path, \"r+b\").read()\n",
        "    video_url = f\"data:video/mp4;base64,{base64.b64encode(video_file).decode()}\"\n",
        "    return ipythondisplay.HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    N_VISUAL_EPISODES = 3\n",
        "    MAX_STEPS_PER_EPISODE = 200\n",
        "    VIDEO_FOLDER = './videos'\n",
        "\n",
        "    print(f\"\\n--- Starting Visualization (Colab Video Recording) ---\")\n",
        "    print(f\"Loading model: {MODEL_SAVE_PATH}\")\n",
        "\n",
        "    base_env = make_env(ENV_ID, violation_penalty=VIOLATION_PENALTY, render_mode='rgb_array')()\n",
        "\n",
        "    viz_env = RecordVideo(base_env, VIDEO_FOLDER, episode_trigger=lambda _: True, name_prefix=f\"ppo-{ENV_ID}\")\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    try:\n",
        "        model = PPO.load(MODEL_SAVE_PATH, device=device)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        viz_env.close()\n",
        "        exit()\n",
        "\n",
        "    for episode in range(N_VISUAL_EPISODES):\n",
        "        print(f\"\\nStarting visualization episode {episode + 1}/{N_VISUAL_EPISODES}\")\n",
        "        obs, info = viz_env.reset()\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        step = 0\n",
        "        total_reward = 0\n",
        "        while not terminated and not truncated and step < MAX_STEPS_PER_EPISODE:\n",
        "            action, _states = model.predict(obs, deterministic=True)\n",
        "            obs, reward, terminated, truncated, info = viz_env.step(action)\n",
        "            total_reward += reward\n",
        "            step += 1\n",
        "        print(f\"Episode {episode+1} finished after {step} steps. Total Reward: {total_reward:.2f}\")\n",
        "\n",
        "    viz_env.close()\n",
        "    print(f\"\\nVisualization finished. Videos saved in {VIDEO_FOLDER}\")\n",
        "\n",
        "    video_files = sorted(Path(VIDEO_FOLDER).glob(\"*.mp4\"))\n",
        "    if video_files:\n",
        "        latest_video = video_files[-1]\n",
        "        print(f\"Displaying video: {latest_video}\")\n",
        "        display(show_video(latest_video))\n",
        "    else:\n",
        "        print(f\"No video files found in {VIDEO_FOLDER}\")"
      ]
    }
  ]
}