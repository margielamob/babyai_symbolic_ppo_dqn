{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4B2pkfVxml-H"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "from pathlib import Path\n",
        "import base64\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from minigrid.wrappers import ImgObsWrapper\n",
        "try:\n",
        "    from minigrid.core.constants import COLOR_NAMES\n",
        "except ImportError:\n",
        "    from minigrid.minigrid_env import COLOR_NAMES\n",
        "\n",
        "\n",
        "from stable_baselines3 import DQN, PPO\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecTransposeImage\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "base_path = \"GITHUB PATH\"\n",
        "DQN_MODEL_PATH = f\"{base_path}dqn_goto_redball_logic_custom_cnn.zip\"\n",
        "PPO_MODEL_PATH = f\"{base_path}ppo_goto_redball_logic_custom_cnn.zip\"\n",
        "\n",
        "print(f\"DQN Path: {Path(DQN_MODEL_PATH)}\")\n",
        "print(f\"PPO Path: {Path(PPO_MODEL_PATH)}\")\n",
        "\n",
        "\n",
        "class GoToRedBallLogicWrapper(gym.Wrapper):\n",
        "    \"\"\"Applies penalties based on symbolic rules for the GoToRedBall task.\"\"\"\n",
        "    def __init__(self, env: gym.Env, violation_penalty: float = -0.5):\n",
        "        super().__init__(env)\n",
        "        self.penalty = violation_penalty\n",
        "        try:\n",
        "            self.red_color_index = COLOR_NAMES.index('red')\n",
        "        except ValueError:\n",
        "             print(f\"Error: 'red' not found in COLOR_NAMES list: {COLOR_NAMES}\")\n",
        "             self.red_color_index = 0\n",
        "        self.red_ball_pos = None\n",
        "        assert hasattr(self.env.unwrapped, 'actions'), \"Environment must have an 'actions' attribute\"\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        grid = self.env.unwrapped.grid\n",
        "        self.red_ball_pos = None\n",
        "        ball_found_debug = False\n",
        "        for i in range(grid.width):\n",
        "            for j in range(grid.height):\n",
        "                cell = grid.get(i, j)\n",
        "                if cell:\n",
        "                    raw_color_attr = getattr(cell, 'color', None)\n",
        "                    cell_color_idx = -1\n",
        "                    if isinstance(raw_color_attr, str):\n",
        "                        try: cell_color_idx = COLOR_NAMES.index(raw_color_attr)\n",
        "                        except ValueError: pass\n",
        "                    elif isinstance(raw_color_attr, int):\n",
        "                         if 0 <= raw_color_attr < len(COLOR_NAMES):\n",
        "                             cell_color_idx = raw_color_attr\n",
        "\n",
        "                    if hasattr(cell, 'type') and cell.type == 'ball':\n",
        "                         ball_found_debug = True\n",
        "                         if cell_color_idx == self.red_color_index:\n",
        "                             self.red_ball_pos = (i, j)\n",
        "                             break\n",
        "            if self.red_ball_pos:\n",
        "                 break\n",
        "        info['logic_violation_count'] = 0\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        logic_violation_type = None\n",
        "        unwrapped_env = self.env.unwrapped\n",
        "\n",
        "        if action == unwrapped_env.actions.pickup:\n",
        "            fx, fy = unwrapped_env.front_pos\n",
        "            cell_in_front = unwrapped_env.grid.get(fx, fy)\n",
        "            if cell_in_front and hasattr(cell_in_front, 'type') and cell_in_front.type == 'ball':\n",
        "                 raw_color_attr = getattr(cell_in_front, 'color', None)\n",
        "                 cell_color_idx = -1\n",
        "                 if isinstance(raw_color_attr, str):\n",
        "                     try: cell_color_idx = COLOR_NAMES.index(raw_color_attr)\n",
        "                     except ValueError: pass\n",
        "                 elif isinstance(raw_color_attr, int):\n",
        "                     if 0 <= raw_color_attr < len(COLOR_NAMES):\n",
        "                        cell_color_idx = raw_color_attr\n",
        "\n",
        "                 if cell_color_idx != self.red_color_index:\n",
        "                    reward += self.penalty\n",
        "                    logic_violation_type = 'pickup_wrong_ball'\n",
        "\n",
        "        if action == unwrapped_env.actions.done:\n",
        "            agent_pos = tuple(unwrapped_env.agent_pos)\n",
        "            if self.red_ball_pos is None:\n",
        "                 reward += self.penalty\n",
        "                 logic_violation_type = 'premature_done_no_target'\n",
        "            elif agent_pos != self.red_ball_pos:\n",
        "                reward += self.penalty\n",
        "                logic_violation_type = 'premature_done'\n",
        "\n",
        "        info['logic_violation'] = logic_violation_type is not None\n",
        "        info['logic_violation_type'] = logic_violation_type\n",
        "\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "\n",
        "class MiniGridCNN(BaseFeaturesExtractor):\n",
        "    \"\"\"Custom CNN Feature Extractor for MiniGrid-like environments (e.g., 7x7 input).\"\"\"\n",
        "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 64):\n",
        "        super().__init__(observation_space, features_dim)\n",
        "        n_input_channels = observation_space.shape[0]\n",
        "        input_height = observation_space.shape[1]\n",
        "        input_width = observation_space.shape[2]\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.as_tensor(observation_space.sample()[None]).float()\n",
        "            n_flatten = self.cnn(dummy_input).shape[1]\n",
        "\n",
        "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
        "\n",
        "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
        "        cnn_output = self.cnn(observations.float())\n",
        "        return self.linear(cnn_output)\n",
        "\n",
        "\n",
        "def make_env(env_id, violation_penalty=-0.5, seed=0, render_mode=None):\n",
        "    \"\"\"Utility function for creating and wrapping the environment.\"\"\"\n",
        "    def _init():\n",
        "        env_kwargs = {'render_mode': render_mode} if render_mode else {}\n",
        "        env = gym.make(env_id, **env_kwargs)\n",
        "        env = ImgObsWrapper(env)\n",
        "        env = GoToRedBallLogicWrapper(env, violation_penalty=violation_penalty)\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "ENV_ID = 'BabyAI-GoToRedBallGrey-v0'\n",
        "FEATURES_DIM = 64\n",
        "VIOLATION_PENALTY = -0.5\n",
        "N_EVAL_EPISODES = 50\n",
        "MAX_STEPS_PER_EPISODE = 200\n",
        "EVAL_SEED = 42\n",
        "\n",
        "def evaluate_model_detailed(model, env_id, n_eval_episodes, max_steps, violation_penalty, seed, features_dim):\n",
        "    \"\"\"\n",
        "    Evaluates an agent, collecting rewards, steps, and logic violations.\n",
        "    Handles both PPO and DQN style observation processing if needed.\n",
        "    \"\"\"\n",
        "    eval_env = make_env(env_id, violation_penalty=violation_penalty, seed=seed)() # Instantiate the callable\n",
        "\n",
        "    is_image_space = isinstance(eval_env.observation_space, gym.spaces.Box) and len(eval_env.observation_space.shape) == 3\n",
        "    transpose_needed = is_image_space\n",
        "\n",
        "    episode_rewards = []\n",
        "    episode_lengths = []\n",
        "    episode_violations = []\n",
        "    total_violations_map = {}\n",
        "\n",
        "    for episode in range(n_eval_episodes):\n",
        "        obs, info = eval_env.reset(seed=seed + episode)\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        step = 0\n",
        "        episode_reward = 0\n",
        "        violations_this_episode = 0\n",
        "\n",
        "        while not terminated and not truncated and step < max_steps:\n",
        "            if transpose_needed:\n",
        "                processed_obs = np.transpose(obs, (2, 0, 1))[None]\n",
        "\n",
        "            else:\n",
        "                processed_obs = obs[None]\n",
        "\n",
        "            action, _states = model.predict(processed_obs, deterministic=True)\n",
        "            action_to_step = action.item() if isinstance(action, (np.ndarray, np.number)) else action\n",
        "\n",
        "            obs, reward, terminated, truncated, info = eval_env.step(action_to_step)\n",
        "\n",
        "            episode_reward += reward\n",
        "            step += 1\n",
        "\n",
        "            if info.get('logic_violation', False):\n",
        "                violations_this_episode += 1\n",
        "                violation_type = info.get('logic_violation_type', 'unknown')\n",
        "                total_violations_map[violation_type] = total_violations_map.get(violation_type, 0) + 1\n",
        "\n",
        "\n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_lengths.append(step)\n",
        "        episode_violations.append(violations_this_episode)\n",
        "\n",
        "    eval_env.close()\n",
        "\n",
        "    mean_reward = np.mean(episode_rewards)\n",
        "    std_reward = np.std(episode_rewards)\n",
        "    mean_length = np.mean(episode_lengths)\n",
        "    std_length = np.std(episode_lengths)\n",
        "    mean_violations = np.mean(episode_violations)\n",
        "    std_violations = np.std(episode_violations)\n",
        "\n",
        "    results = {\n",
        "        \"mean_reward\": mean_reward,\n",
        "        \"std_reward\": std_reward,\n",
        "        \"mean_length\": mean_length,\n",
        "        \"std_length\": std_length,\n",
        "        \"mean_violations\": mean_violations,\n",
        "        \"std_violations\": std_violations,\n",
        "        \"total_violations_by_type\": total_violations_map,\n",
        "        \"all_rewards\": episode_rewards\n",
        "    }\n",
        "    return results\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "custom_objects_dqn = {\n",
        "    \"policy_kwargs\": dict(\n",
        "        features_extractor_class=MiniGridCNN,\n",
        "        features_extractor_kwargs=dict(features_dim=FEATURES_DIM),\n",
        "    ),\n",
        "}\n",
        "custom_objects_ppo = {\n",
        "     \"policy_kwargs\": dict(\n",
        "        features_extractor_class=MiniGridCNN,\n",
        "        features_extractor_kwargs=dict(features_dim=FEATURES_DIM),\n",
        "    )\n",
        "}\n",
        "\n",
        "model_dqn = None\n",
        "model_ppo = None\n",
        "\n",
        "print(\"\\nLoading DQN model...\")\n",
        "try:\n",
        "    if Path(DQN_MODEL_PATH).exists():\n",
        "         temp_env_dqn = make_vec_env(lambda: make_env(ENV_ID, violation_penalty=VIOLATION_PENALTY)(), n_envs=1, vec_env_cls=DummyVecEnv)\n",
        "         temp_env_dqn = VecTransposeImage(temp_env_dqn)\n",
        "\n",
        "         model_dqn = DQN.load(DQN_MODEL_PATH, env=temp_env_dqn, custom_objects=custom_objects_dqn, device=device)\n",
        "         print(\"DQN model loaded successfully.\")\n",
        "    else:\n",
        "        print(f\"DQN model file not found at {DQN_MODEL_PATH}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading DQN model: {e}\")\n",
        "    if 'temp_env_dqn' in locals() and temp_env_dqn is not None:\n",
        "        temp_env_dqn.close()\n",
        "\n",
        "\n",
        "print(\"\\nLoading PPO model...\")\n",
        "try:\n",
        "    if Path(PPO_MODEL_PATH).exists():\n",
        "         temp_env_ppo = make_vec_env(lambda: make_env(ENV_ID, violation_penalty=VIOLATION_PENALTY)(), n_envs=1, vec_env_cls=DummyVecEnv)\n",
        "         temp_env_ppo = VecTransposeImage(temp_env_ppo)\n",
        "\n",
        "         model_ppo = PPO.load(PPO_MODEL_PATH, env=temp_env_ppo, custom_objects=custom_objects_ppo, device=device)\n",
        "         print(\"PPO model loaded successfully.\")\n",
        "    else:\n",
        "        print(f\"PPO model file not found at {PPO_MODEL_PATH}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading PPO model: {e}\")\n",
        "    if 'temp_env_ppo' in locals() and temp_env_ppo is not None:\n",
        "         temp_env_ppo.close()\n",
        "\n",
        "\n",
        "results_dqn = None\n",
        "results_ppo = None\n",
        "\n",
        "if model_dqn:\n",
        "    print(f\"\\nEvaluating DQN model for {N_EVAL_EPISODES} episodes...\")\n",
        "    start_time = time.time()\n",
        "    results_dqn = evaluate_model_detailed(\n",
        "        model_dqn, ENV_ID, N_EVAL_EPISODES, MAX_STEPS_PER_EPISODE, VIOLATION_PENALTY, EVAL_SEED, FEATURES_DIM\n",
        "    )\n",
        "    print(f\"DQN Evaluation finished in {time.time() - start_time:.2f} seconds.\")\n",
        "else:\n",
        "    print(\"\\nSkipping DQN evaluation as model failed to load.\")\n",
        "\n",
        "if model_ppo:\n",
        "    print(f\"\\nEvaluating PPO model for {N_EVAL_EPISODES} episodes...\")\n",
        "    start_time = time.time()\n",
        "    results_ppo = evaluate_model_detailed(\n",
        "        model_ppo, ENV_ID, N_EVAL_EPISODES, MAX_STEPS_PER_EPISODE, VIOLATION_PENALTY, EVAL_SEED + N_EVAL_EPISODES, FEATURES_DIM # Use different seed offset\n",
        "    )\n",
        "    print(f\"PPO Evaluation finished in {time.time() - start_time:.2f} seconds.\")\n",
        "else:\n",
        "    print(\"\\nSkipping PPO evaluation as model failed to load.\")\n",
        "\n",
        "\n",
        "comparison_data = []\n",
        "if results_dqn:\n",
        "    comparison_data.append({\n",
        "        \"Model\": \"DQN\",\n",
        "        \"Mean Reward\": results_dqn['mean_reward'],\n",
        "        \"Std Reward\": results_dqn['std_reward'],\n",
        "        \"Mean Length\": results_dqn['mean_length'],\n",
        "        \"Std Length\": results_dqn['std_length'],\n",
        "        \"Mean Violations\": results_dqn['mean_violations'],\n",
        "        \"Violation Details\": results_dqn['total_violations_by_type']\n",
        "    })\n",
        "if results_ppo:\n",
        "     comparison_data.append({\n",
        "        \"Model\": \"PPO\",\n",
        "        \"Mean Reward\": results_ppo['mean_reward'],\n",
        "        \"Std Reward\": results_ppo['std_reward'],\n",
        "        \"Mean Length\": results_ppo['mean_length'],\n",
        "        \"Std Length\": results_ppo['std_length'],\n",
        "        \"Mean Violations\": results_ppo['mean_violations'],\n",
        "        \"Violation Details\": results_ppo['total_violations_by_type']\n",
        "    })\n",
        "\n",
        "if comparison_data:\n",
        "    df_comparison = pd.DataFrame(comparison_data)\n",
        "    df_comparison.set_index(\"Model\", inplace=True)\n",
        "\n",
        "    print(\"\\n\\n--- Performance Comparison ---\")\n",
        "    float_cols = [\"Mean Reward\", \"Std Reward\", \"Mean Length\", \"Std Length\", \"Mean Violations\"]\n",
        "    for col in float_cols:\n",
        "        if col in df_comparison.columns:\n",
        "             df_comparison[col] = df_comparison[col].map('{:.2f}'.format)\n",
        "\n",
        "    print(df_comparison)\n",
        "\n",
        "    plot_data = []\n",
        "    if results_dqn:\n",
        "        for r in results_dqn['all_rewards']:\n",
        "            plot_data.append({\"Model\": \"DQN\", \"Episode Reward\": r})\n",
        "    if results_ppo:\n",
        "        for r in results_ppo['all_rewards']:\n",
        "             plot_data.append({\"Model\": \"PPO\", \"Episode Reward\": r})\n",
        "\n",
        "    if plot_data:\n",
        "        df_plot = pd.DataFrame(plot_data)\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.boxplot(x=\"Model\", y=\"Episode Reward\", data=df_plot)\n",
        "        plt.title(f'Reward Distribution per Episode ({N_EVAL_EPISODES} Episodes)')\n",
        "        plt.ylabel(\"Total Reward (including penalties)\")\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"\\nNo data available for plotting reward distribution.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo evaluation results to display.\")\n",
        "\n",
        "\n",
        "print(\"\\nComparison script finished.\")"
      ]
    }
  ]
}