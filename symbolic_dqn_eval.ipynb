{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUOT5eaenLyY"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import base64\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from minigrid.wrappers import ImgObsWrapper\n",
        "try:\n",
        "    from minigrid.core.constants import COLOR_NAMES\n",
        "except ImportError:\n",
        "        from minigrid.minigrid_env import COLOR_NAMES\n",
        "\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecTransposeImage\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "\n",
        "# Copied from training but for dqn\n",
        "\n",
        "class GoToRedBallLogicWrapper(gym.Wrapper):\n",
        "    def __init__(self, env: gym.Env, violation_penalty: float = -0.5):\n",
        "        super().__init__(env)\n",
        "        self.penalty = violation_penalty\n",
        "        try:\n",
        "            self.red_color_index = COLOR_NAMES.index('red')\n",
        "        except ValueError:\n",
        "             print(f\"Error: 'red' not found in COLOR_NAMES list: {COLOR_NAMES}\")\n",
        "             self.red_color_index = 0\n",
        "        self.red_ball_pos = None\n",
        "        assert hasattr(self.env.unwrapped, 'actions'), \"Environment must have an 'actions' attribute\"\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        grid = self.env.unwrapped.grid\n",
        "        self.red_ball_pos = None\n",
        "        ball_found_debug = False\n",
        "\n",
        "        for i in range(grid.width):\n",
        "            for j in range(grid.height):\n",
        "                cell = grid.get(i, j)\n",
        "                if cell:\n",
        "                    raw_color_attr = getattr(cell, 'color', None)\n",
        "                    cell_type = getattr(cell, 'type', 'N/A')\n",
        "                    cell_color_idx = -1\n",
        "                    if isinstance(raw_color_attr, str):\n",
        "                        try: cell_color_idx = COLOR_NAMES.index(raw_color_attr)\n",
        "                        except ValueError: pass\n",
        "                    elif isinstance(raw_color_attr, int):\n",
        "                         if 0 <= raw_color_attr < len(COLOR_NAMES):\n",
        "                             cell_color_idx = raw_color_attr\n",
        "\n",
        "                    if hasattr(cell, 'type') and cell.type == 'ball':\n",
        "                         ball_found_debug = True\n",
        "                         if cell_color_idx == self.red_color_index:\n",
        "                             self.red_ball_pos = (i, j)\n",
        "                             break\n",
        "            if self.red_ball_pos:\n",
        "                 break\n",
        "\n",
        "        if not self.red_ball_pos:\n",
        "            print(f\"Eval Warning: Red ball not found during reset! (Any ball objects found: {ball_found_debug})\")\n",
        "\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        info['logic_violation'] = None\n",
        "        unwrapped_env = self.env.unwrapped\n",
        "\n",
        "        if action == unwrapped_env.actions.pickup:\n",
        "            fx, fy = unwrapped_env.front_pos\n",
        "            cell_in_front = unwrapped_env.grid.get(fx, fy)\n",
        "            if cell_in_front and hasattr(cell_in_front, 'type') and cell_in_front.type == 'ball':\n",
        "                 raw_color_attr = getattr(cell_in_front, 'color', None)\n",
        "                 cell_color_idx = -1\n",
        "                 if isinstance(raw_color_attr, str):\n",
        "                     try: cell_color_idx = COLOR_NAMES.index(raw_color_attr)\n",
        "                     except ValueError: pass\n",
        "                 elif isinstance(raw_color_attr, int):\n",
        "                     if 0 <= raw_color_attr < len(COLOR_NAMES):\n",
        "                        cell_color_idx = raw_color_attr\n",
        "\n",
        "                 if cell_color_idx != self.red_color_index:\n",
        "                    reward += self.penalty\n",
        "                    info['logic_violation'] = 'pickup_wrong_ball'\n",
        "\n",
        "        if action == unwrapped_env.actions.done:\n",
        "            agent_pos = tuple(unwrapped_env.agent_pos)\n",
        "            if self.red_ball_pos is None:\n",
        "                 reward += self.penalty\n",
        "                 info['logic_violation'] = 'premature_done_no_target'\n",
        "            elif agent_pos != self.red_ball_pos:\n",
        "                reward += self.penalty\n",
        "                info['logic_violation'] = 'premature_done'\n",
        "\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "\n",
        "class MiniGridCNN(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 64):\n",
        "        super().__init__(observation_space, features_dim)\n",
        "        n_input_channels = observation_space.shape[0]\n",
        "        print(f\"Initializing MiniGridCNN with expected CHW input shape similar to: ({n_input_channels}, H, W)\")\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        obs_shape = observation_space.shape\n",
        "        h = obs_shape[1]\n",
        "        w = obs_shape[2]\n",
        "        dummy_input = torch.zeros(1, n_input_channels, h, w)\n",
        "        with torch.no_grad():\n",
        "            n_flatten = self.cnn(dummy_input).shape[1]\n",
        "            print(f\"Calculated flattened CNN output size: {n_flatten}\")\n",
        "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
        "\n",
        "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
        "        return self.linear(self.cnn(observations.float()))\n",
        "\n",
        "def make_env(env_id, violation_penalty=-0.5, seed=0, render_mode=None):\n",
        "    def _init():\n",
        "        env_kwargs = {'render_mode': render_mode} if render_mode else {}\n",
        "        env = gym.make(env_id, **env_kwargs)\n",
        "        env = ImgObsWrapper(env)\n",
        "        env = GoToRedBallLogicWrapper(env, violation_penalty=violation_penalty)\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "ENV_ID = 'BabyAI-GoToRedBallGrey-v0'\n",
        "MODEL_SAVE_PATH = \"dqn_goto_redball_logic_custom_cnn.zip\"\n",
        "VIOLATION_PENALTY = -0.5\n",
        "FEATURES_DIM = 64\n",
        "\n",
        "# EVALUATION SCRIPT\n",
        "def run_evaluation():\n",
        "    N_EVAL_EPISODES = 50\n",
        "    EVAL_NUM_ENVS = 1\n",
        "\n",
        "    print(f\"\\n--- Starting Evaluation ---\")\n",
        "    print(f\"Loading model: {MODEL_SAVE_PATH}\")\n",
        "    print(f\"Evaluating on env: {ENV_ID} for {N_EVAL_EPISODES} episodes.\")\n",
        "\n",
        "    eval_env = DummyVecEnv([make_env(ENV_ID, violation_penalty=VIOLATION_PENALTY, seed=1000)])\n",
        "    eval_env = VecTransposeImage(eval_env)\n",
        "\n",
        "    try:\n",
        "        custom_objects = {\n",
        "            \"features_extractor_class\": MiniGridCNN,\n",
        "            \"features_extractor_kwargs\": dict(features_dim=FEATURES_DIM),\n",
        "        }\n",
        "        model = DQN.load(MODEL_SAVE_PATH, env=eval_env, custom_objects=custom_objects, device='auto') # <-- Changed PPO to DQN\n",
        "        print(\"DQN model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        print(\"Ensure the model file exists, custom objects are correct, and dependencies match.\")\n",
        "        eval_env.close()\n",
        "        return\n",
        "\n",
        "    mean_reward, std_reward = evaluate_policy(\n",
        "        model,\n",
        "        model.get_env(),\n",
        "        n_eval_episodes=N_EVAL_EPISODES,\n",
        "        deterministic=True,\n",
        "        render=False\n",
        "    )\n",
        "\n",
        "    print(f\"\\n--- Evaluation Results ---\")\n",
        "    print(f\"Mean reward over {N_EVAL_EPISODES} episodes: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "    print(\"Note: 'Reward' here includes penalties from the wrapper.\")\n",
        "\n",
        "    eval_env.close()\n",
        "    if model.get_env() is not eval_env and model.get_env() is not None:\n",
        "         model.get_env().close()\n",
        "    print(\"\\nEvaluation finished.\")\n",
        "\n",
        "# VISUALIZATION SCRIPT\n",
        "\n",
        "def show_video(video_path, video_width = 600):\n",
        "    video_path = Path(video_path)\n",
        "    if not video_path.is_file():\n",
        "        print(f\"Error: Video file not found at {video_path}\")\n",
        "        return\n",
        "    video_file = open(video_path, \"r+b\").read()\n",
        "    video_url = f\"data:video/mp4;base64,{base64.b64encode(video_file).decode()}\"\n",
        "    return ipythondisplay.HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")\n",
        "\n",
        "def run_visualization():\n",
        "    N_VISUAL_EPISODES = 3\n",
        "    MAX_STEPS_PER_EPISODE = 200\n",
        "    VIDEO_FOLDER = './videos_dqn'\n",
        "    Path(VIDEO_FOLDER).mkdir(exist_ok=True)\n",
        "\n",
        "    print(f\"\\n--- Starting Visualization (Colab Video Recording) ---\")\n",
        "    print(f\"Loading model: {MODEL_SAVE_PATH}\")\n",
        "\n",
        "    base_env_callable = make_env(ENV_ID, violation_penalty=VIOLATION_PENALTY, render_mode='rgb_array')\n",
        "    base_env = base_env_callable()\n",
        "\n",
        "    viz_env = RecordVideo(base_env, VIDEO_FOLDER, episode_trigger=lambda ep_id: True, name_prefix=f\"dqn-{ENV_ID}\") # <-- Changed name_prefix\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    try:\n",
        "        model = DQN.load(MODEL_SAVE_PATH, device=device)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        viz_env.close()\n",
        "        return\n",
        "\n",
        "    obs_space = viz_env.observation_space\n",
        "    is_image_space = isinstance(obs_space, gym.spaces.Box) and len(obs_space.shape) == 3\n",
        "    transpose_order = (2, 0, 1)\n",
        "\n",
        "\n",
        "    for episode in range(N_VISUAL_EPISODES):\n",
        "        print(f\"\\nStarting visualization episode {episode + 1}/{N_VISUAL_EPISODES}\")\n",
        "        obs, info = viz_env.reset()\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        step = 0\n",
        "        total_reward = 0\n",
        "\n",
        "        while not terminated and not truncated and step < MAX_STEPS_PER_EPISODE:\n",
        "            if is_image_space:\n",
        "                 processed_obs = np.transpose(obs, transpose_order)[None]\n",
        "            else:\n",
        "                 processed_obs = obs[None]\n",
        "\n",
        "            action, _states = model.predict(processed_obs, deterministic=True)\n",
        "            obs, reward, terminated, truncated, info = viz_env.step(action.item())\n",
        "\n",
        "            total_reward += reward\n",
        "            step += 1\n",
        "\n",
        "        print(f\"Episode {episode+1} finished after {step} steps. Total Reward: {total_reward:.2f}\")\n",
        "\n",
        "    viz_env.close()\n",
        "    print(f\"\\nVisualization finished. Videos saved in {VIDEO_FOLDER}\")\n",
        "\n",
        "    try:\n",
        "        video_files = sorted(Path(VIDEO_FOLDER).glob(\"*.mp4\"))\n",
        "        if video_files:\n",
        "            latest_video = video_files[-1]\n",
        "            print(f\"Displaying video: {latest_video}\")\n",
        "            display(show_video(latest_video))\n",
        "        else:\n",
        "            print(f\"No video files found in {VIDEO_FOLDER}\")\n",
        "    except Exception as e:\n",
        "         print(f\"Could not display video: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_evaluation()\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "    run_visualization()"
      ]
    }
  ]
}