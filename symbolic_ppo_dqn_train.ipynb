{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from minigrid.wrappers import ImgObsWrapper\n",
        "from minigrid.core.constants import COLOR_NAMES\n",
        "\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "import numpy as np\n",
        "\n",
        "# 1. Custom Logic Wrapper because we use 7x7\n",
        "class GoToRedBallLogicWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    Applies penalties based on symbolic rules for the GoToRedBall task.\n",
        "    - Penalizes picking up non-red balls.\n",
        "    - Penalizes calling 'done' when not at the red ball location.\n",
        "    \"\"\"\n",
        "    def __init__(self, env: gym.Env, violation_penalty: float = -0.5):\n",
        "        super().__init__(env)\n",
        "        self.penalty = violation_penalty\n",
        "        self.red_color_index = COLOR_NAMES.index('red')\n",
        "        self.red_ball_pos = None\n",
        "        assert hasattr(self.env.unwrapped, 'actions'), \"Environment must have an 'actions' attribute\"\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        grid = self.env.unwrapped.grid\n",
        "        self.red_ball_pos = None\n",
        "\n",
        "        print(f\"\\n--- RESETTING ENV --- Grid Size: {grid.width}x{grid.height}\")\n",
        "        self.red_ball_pos = None\n",
        "        ball_found_debug = False\n",
        "\n",
        "        for i in range(grid.width):\n",
        "            for j in range(grid.height):\n",
        "                cell = grid.get(i, j)\n",
        "                if cell:\n",
        "                    raw_color_attr = getattr(cell, 'color', None)\n",
        "                    cell_type = getattr(cell, 'type', 'N/A')\n",
        "                    cell_color_idx = -1\n",
        "\n",
        "                    if isinstance(raw_color_attr, str):\n",
        "                        try: cell_color_idx = COLOR_NAMES.index(raw_color_attr)\n",
        "                        except ValueError: pass\n",
        "                    elif isinstance(raw_color_attr, int):\n",
        "                        if 0 <= raw_color_attr < len(COLOR_NAMES):\n",
        "                          cell_color_idx = raw_color_attr\n",
        "                    if hasattr(cell, 'type') and cell.type == 'ball':\n",
        "                        ball_found_debug = True\n",
        "                        if cell_color_idx == self.red_color_index:\n",
        "                            self.red_ball_pos = (i, j)\n",
        "                            break\n",
        "            if self.red_ball_pos:\n",
        "                break\n",
        "\n",
        "        if not self.red_ball_pos:\n",
        "            print(f\"Warning: Red ball not found during reset! (Any ball objects found: {ball_found_debug})\")\n",
        "        else:\n",
        "            print(f\"FOUND: Red ball successfully located at {self.red_ball_pos}\")\n",
        "        print(\"--- RESET COMPLETE ---\")\n",
        "\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        info['logic_violation'] = None\n",
        "\n",
        "        unwrapped_env = self.env.unwrapped\n",
        "\n",
        "        if action == unwrapped_env.actions.pickup:\n",
        "            fx, fy = unwrapped_env.front_pos\n",
        "            cell_in_front = unwrapped_env.grid.get(fx, fy)\n",
        "            if cell_in_front and hasattr(cell_in_front, 'type') and cell_in_front.type == 'ball' and hasattr(cell_in_front, 'color') and cell_in_front.color != self.red_color_index:\n",
        "                reward += self.penalty\n",
        "                info['logic_violation'] = 'pickup_wrong_ball'\n",
        "\n",
        "        if action == unwrapped_env.actions.done:\n",
        "            agent_pos = tuple(unwrapped_env.agent_pos)\n",
        "            if self.red_ball_pos is None:\n",
        "                 print(\"Warning: Checking done action but red_ball_pos is None.\")\n",
        "            elif agent_pos != self.red_ball_pos:\n",
        "                reward += self.penalty\n",
        "                info['logic_violation'] = 'premature_done'\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "# 2. Custom CNN Feature Extractor\n",
        "class MiniGridCNN(BaseFeaturesExtractor):\n",
        "    \"\"\"\n",
        "    Custom CNN Feature Extractor for MiniGrid-like environments (7x7 input).\n",
        "    \"\"\"\n",
        "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 64):\n",
        "        super().__init__(observation_space, features_dim)\n",
        "        n_input_channels = observation_space.shape[0]\n",
        "        input_height = observation_space.shape[1]\n",
        "        input_width = observation_space.shape[2]\n",
        "        print(f\"Initializing MiniGridCNN with input shape: ({n_input_channels}, {input_height}, {input_width})\")\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.as_tensor(observation_space.sample()[None]).float()\n",
        "            n_flatten = self.cnn(dummy_input).shape[1]\n",
        "            print(f\"Flattened CNN output size: {n_flatten}\")\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(n_flatten, features_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
        "        cnn_output = self.cnn(observations.float())\n",
        "        return self.linear(cnn_output)\n",
        "\n",
        "def make_env(env_id='BabyAI-GoToRedBallGrey-v0', violation_penalty=-0.5, seed=0):\n",
        "    \"\"\"\n",
        "    Utility function for creating and wrapping the environment.\n",
        "    Includes seeding for reproducibility.\n",
        "    \"\"\"\n",
        "    def _init():\n",
        "        env = gym.make(env_id, render_mode='rgb_array')\n",
        "        env = ImgObsWrapper(env)\n",
        "        env = GoToRedBallLogicWrapper(env, violation_penalty=violation_penalty)\n",
        "        return env\n",
        "    return _init"
      ],
      "metadata": {
        "id": "owEn4ZE7746d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "# 4. Training for PPO\n",
        "if __name__ == \"__main__\":\n",
        "    ENV_ID = 'BabyAI-GoToRedBallGrey-v0'\n",
        "    NUM_ENVS = 4\n",
        "    LOG_DIR = \"./logs/gotorball_logic_custom_cnn/\"\n",
        "    MODEL_SAVE_PATH = \"ppo_goto_redball_logic_custom_cnn\"\n",
        "    TOTAL_TIMESTEPS = 1_000_000\n",
        "    FEATURES_DIM = 64\n",
        "    VIOLATION_PENALTY = -0.5\n",
        "\n",
        "    env_fns = [make_env(ENV_ID, violation_penalty=VIOLATION_PENALTY, seed=i) for i in range(NUM_ENVS)]\n",
        "    vec_env = DummyVecEnv(env_fns)\n",
        "\n",
        "    print(f\"Observation Space (after VecTransposeImage): {vec_env.observation_space.shape}\")\n",
        "    print(f\"Action Space: {vec_env.action_space}\")\n",
        "\n",
        "    policy_kwargs = dict(\n",
        "        features_extractor_class=MiniGridCNN,\n",
        "        features_extractor_kwargs=dict(features_dim=FEATURES_DIM),\n",
        "    )\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = PPO(\n",
        "        policy='CnnPolicy',\n",
        "        env=vec_env,\n",
        "        policy_kwargs=policy_kwargs,\n",
        "        verbose=1,\n",
        "        tensorboard_log=LOG_DIR,\n",
        "        device=device,\n",
        "        n_steps=512,\n",
        "        batch_size=64 * NUM_ENVS,\n",
        "        n_epochs=4,\n",
        "        gamma=0.99,\n",
        "        gae_lambda=0.95,\n",
        "        clip_range=0.2,\n",
        "        ent_coef=0.02,\n",
        "        vf_coef=0.5,\n",
        "        learning_rate=1e-3,\n",
        "    )\n",
        "\n",
        "    print(f\"\\nStarting training on {ENV_ID} for {TOTAL_TIMESTEPS} timesteps...\")\n",
        "    print(f\"TensorBoard logs will be saved to: {LOG_DIR}\")\n",
        "    print(f\"Model will be saved to: {MODEL_SAVE_PATH}.zip\\n\")\n",
        "\n",
        "\n",
        "    model.learn(\n",
        "        total_timesteps=TOTAL_TIMESTEPS,\n",
        "        log_interval=10,\n",
        "        )\n",
        "\n",
        "    model.save(MODEL_SAVE_PATH)\n",
        "    print(f\"\\nTraining finished. Model saved to {MODEL_SAVE_PATH}.zip\")\n",
        "\n",
        "    vec_env.close()\n",
        "    print(\"Environment closed.\")"
      ],
      "metadata": {
        "id": "rg5HylmGPWur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecTransposeImage\n",
        "# 4. Training for DQN\n",
        "if __name__ == \"__main__\":\n",
        "    ENV_ID = 'BabyAI-GoToRedBallGrey-v0'\n",
        "    NUM_ENVS = 1\n",
        "    LOG_DIR = \"./logs/gotorball_logic_custom_cnn_dqn/\"\n",
        "    MODEL_SAVE_PATH = \"dqn_goto_redball_logic_custom_cnn\"\n",
        "    TOTAL_TIMESTEPS = 1_000_000\n",
        "    FEATURES_DIM = 64\n",
        "    VIOLATION_PENALTY = -0.5\n",
        "\n",
        "    vec_env = make_vec_env(\n",
        "        lambda: make_env(ENV_ID, violation_penalty=VIOLATION_PENALTY, seed=0)(),\n",
        "        n_envs=NUM_ENVS,\n",
        "        vec_env_cls=DummyVecEnv\n",
        "    )\n",
        "    vec_env = VecTransposeImage(vec_env)\n",
        "\n",
        "\n",
        "    print(f\"Observation Space: {vec_env.observation_space.shape}\")\n",
        "    print(f\"Action Space: {vec_env.action_space}\")\n",
        "\n",
        "    policy_kwargs = dict(\n",
        "        features_extractor_class=MiniGridCNN,\n",
        "        features_extractor_kwargs=dict(features_dim=FEATURES_DIM),\n",
        "    )\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = DQN(\n",
        "        policy='CnnPolicy',\n",
        "        env=vec_env,\n",
        "        policy_kwargs=policy_kwargs,\n",
        "        verbose=1,\n",
        "        tensorboard_log=LOG_DIR,\n",
        "        device=device,\n",
        "        buffer_size=100_000,\n",
        "        learning_rate=1e-4,\n",
        "        learning_starts=5000,\n",
        "        batch_size=32,\n",
        "        tau=1.0,\n",
        "        gamma=0.99,\n",
        "        train_freq=(4, \"step\"),\n",
        "        gradient_steps=1,\n",
        "        target_update_interval=1000,\n",
        "        exploration_fraction=0.1,\n",
        "        exploration_initial_eps=1.0,\n",
        "        exploration_final_eps=0.05,\n",
        "        optimize_memory_usage=False,\n",
        "    )\n",
        "\n",
        "    print(f\"\\nStarting DQN training on {ENV_ID} for {TOTAL_TIMESTEPS} timesteps...\")\n",
        "    print(f\"TensorBoard logs will be saved to: {LOG_DIR}\")\n",
        "    print(f\"Model will be saved to: {MODEL_SAVE_PATH}.zip\\n\")\n",
        "\n",
        "\n",
        "    model.learn(\n",
        "        total_timesteps=TOTAL_TIMESTEPS,\n",
        "        log_interval=4\n",
        "    )\n",
        "\n",
        "    model.save(MODEL_SAVE_PATH)\n",
        "    print(f\"\\nTraining finished. Model saved to {MODEL_SAVE_PATH}.zip\")\n",
        "\n",
        "    vec_env.close()\n",
        "    print(\"Environment closed.\")"
      ],
      "metadata": {
        "id": "NO84-OtYTLqa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}